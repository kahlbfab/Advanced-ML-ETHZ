{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "82eyOCwbsCqr"
      },
      "outputs": [],
      "source": [
        "!pip install segmentation_models_pytorch\n",
        "!pip install git+https://github.com/albumentations-team/albumentations\n",
        "!pip install wandb"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "import gzip\n",
        "import numpy as np\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import random\n",
        "\n",
        "import cv2\n",
        "import albumentations as A\n",
        "from albumentations.augmentations.geometric.transforms import ElasticTransform, Affine, GridDistortion, PadIfNeeded\n",
        "from albumentations.augmentations.crops.transforms import CenterCrop\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "\n",
        "import torchvision.transforms as T\n",
        "from PIL import Image\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "from segmentation_models_pytorch.losses import DiceLoss, JaccardLoss\n",
        "import segmentation_models_pytorch as smp\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "PATH = \"drive/My Drive/aml_task3/data\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2eG0Ll7PsjO0",
        "outputId": "5c5cedfa-b486-40e0-fe7d-3bbaf7a0b522"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if USE_WANDB:\n",
        "  !wandb login\n",
        "  import wandb\n",
        "  wandb.init(project=\"heart_valve_segmentation\", name=PRED_FILE_NAME)"
      ],
      "metadata": {
        "id": "J9DPzbHqsUtt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# PARAMETERS --------------\n",
        "USE_WANDB = True\n",
        "PRED_FILE_NAME = 'prediction'\n",
        "\n",
        "# Preprocessing\n",
        "NBR_OVERSAMPLINGS = 1\n",
        "PAD_EXPERT_IMAGES = False\n",
        "RESIZE_SIZE = 448\n",
        "BATCH_SIZE = 16\n",
        "PROBABILITY_AUGMENTATION = 0.6\n",
        "\n",
        "# Model\n",
        "ENCODER_NAME = 'resnet18'\n",
        "ENCODER_WEIGHTS = None\n",
        "ENCODER_DEPTH = 5\n",
        "DECODER_ATTENTION_TYPE = None\n",
        "MODEL_TYPE = 'Unet++'"
      ],
      "metadata": {
        "id": "ux6SWu1CGaKg"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# helper functions --------------\n",
        "def load_zipped_pickle(filename):\n",
        "    with gzip.open(filename, 'rb') as f:\n",
        "        loaded_object = pickle.load(f)\n",
        "        return loaded_object\n",
        "\n",
        "  \n",
        "def save_zipped_pickle(obj, filename):\n",
        "    with gzip.open(filename, 'wb') as f:\n",
        "        pickle.dump(obj, f, 2)\n",
        "\n",
        "\n",
        "def show_side_by_side(video, mask):\n",
        "    fig = plt.figure()\n",
        "    fig.add_subplot(1, 2, 1)\n",
        "    plt.imshow(video)\n",
        "    fig.add_subplot(1, 2, 2)\n",
        "    plt.imshow(mask)\n",
        "\n",
        "\n",
        "def show_on_top(image1, image2, alpha1=0.5, alpha2=0.5):\n",
        "  plt.imshow(image1, alpha = alpha1)\n",
        "  plt.imshow(image2, alpha = alpha2)"
      ],
      "metadata": {
        "id": "AUMrtJdHskH4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class HeartData(Dataset):\n",
        "    \"\"\" \n",
        "    Custom pytorch dataset for heart data:\n",
        "    - pad_expert_images = 'if True the expert images are padded to a square shape to preserve the original aspect-ratio after resizing';\n",
        "    - oversample = 'if True the expert images are triplicated';\n",
        "    - idx = 'list of video indices to read. This is useful to make a train-validation split.'\n",
        "    \"\"\"\n",
        "    def __init__(self, file, transform=None, pad_expert_images=False, nbr_oversamplings=0, idx=list(range(65))):\n",
        "        # load data into memory\n",
        "        data = load_zipped_pickle(file)\n",
        "        # extract labelled frames and masks\n",
        "        self.images = []\n",
        "        self.masks = []\n",
        "        padder = PadIfNeeded(min_height=863, min_width=863, border_mode=cv2.BORDER_CONSTANT, value=0)\n",
        "        for i in range(len(data)):\n",
        "          if i in idx: # read only provided indices\n",
        "            sample = data[i]\n",
        "            for frame_idx, frame in enumerate(sample[\"frames\"]):\n",
        "                video_frame = sample[\"video\"][:, :, frame]\n",
        "                mask_frame = sample[\"label\"][:, :, frame]\n",
        "                # padding\n",
        "                if pad_expert_images and sample['dataset'] == 'expert':\n",
        "                  padded_frame = padder(image=video_frame, mask=mask_frame.astype(int))\n",
        "                  video_frame = padded_frame['image']\n",
        "                  mask_frame = padded_frame['mask']\n",
        "                # first append\n",
        "                self.images.append(video_frame)\n",
        "                self.masks.append(mask_frame)\n",
        "                # oversampling\n",
        "                if nbr_oversamplings and sample[\"dataset\"] == \"expert\":\n",
        "                  for _ in range(nbr_oversamplings):\n",
        "                    self.images.append(video_frame)\n",
        "                    self.masks.append(mask_frame)\n",
        "        # store the transformation\n",
        "        self.transform = transform\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        # fetch\n",
        "        image = self.images[idx]\n",
        "        mask = self.masks[idx]\n",
        "        # transform\n",
        "        if self.transform:\n",
        "            transformed = self.transform(image=image, mask=mask.astype(int))\n",
        "            image = transformed['image']\n",
        "            mask = transformed['mask']\n",
        "        # move to GPU\n",
        "        device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "        image, mask = image.to(device), mask.to(device)\n",
        "        return image, mask"
      ],
      "metadata": {
        "id": "IldoPke3zbAy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# transformation definition ----------------------------------------------\n",
        "train_transform = A.Compose([\n",
        "    # augmentation \n",
        "    A.OneOf([\n",
        "        A.ElasticTransform(alpha=15, sigma=8, alpha_affine=5, p=0.5),\n",
        "        Affine(\n",
        "           scale=(0.95, 1.05), # zoom in/out\n",
        "           translate_percent={'x':(-0.05, 0.03), 'y':(-0.03, 0.1)}, # shift x 5% to left 3% to right\n",
        "           rotate=(-10, 10), \n",
        "           shear=(-10, 10), # change in perspective\n",
        "           keep_ratio=True,\n",
        "           p=0.5\n",
        "        )], p=PROBABILITY_AUGMENTATION\n",
        "    ),    \n",
        "    GridDistortion(num_steps=5, distort_limit=0.2, p=PROBABILITY_AUGMENTATION),\n",
        "    # contrast (improving on histogram equalization)\n",
        "    A.CLAHE(p=1), \n",
        "    # downstream compatibility\n",
        "    A.Resize(RESIZE_SIZE, RESIZE_SIZE),\n",
        "    A.ToRGB(),\n",
        "    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "    ToTensorV2()\n",
        "])\n",
        "\n",
        "val_transform = A.Compose([\n",
        "    # contrast (improving on histogram equalization)\n",
        "    A.CLAHE(p=1),\n",
        "    # downstream compatibility\n",
        "    A.Resize(RESIZE_SIZE, RESIZE_SIZE),\n",
        "    A.ToRGB(),\n",
        "    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "    ToTensorV2()\n",
        "])"
      ],
      "metadata": {
        "id": "sdfV_PEAszKB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create train/val split\n",
        "train_idx = random.sample(range(65), 52)\n",
        "val_idx = list(set(range(65)) - set(train_idx))\n",
        "\n",
        "# instantiate data sets\n",
        "train_dataset = HeartData(\n",
        "    \"{}/train.pkl\".format(PATH),\n",
        "    train_transform,\n",
        "    pad_expert_images=PAD_EXPERT_IMAGES,\n",
        "    nbr_oversamplings=NBR_OVERSAMPLINGS,\n",
        "    idx=train_idx\n",
        ")\n",
        "\n",
        "val_dataset = HeartData(\n",
        "    \"{}/train.pkl\".format(PATH),\n",
        "    val_transform,\n",
        "    pad_expert_images=PAD_EXPERT_IMAGES,\n",
        "    nbr_oversamplings=False, # don't oversample in the validation set!\n",
        "    idx=val_idx\n",
        ")\n",
        "\n",
        "# instantiate loaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=True)"
      ],
      "metadata": {
        "id": "zAe9zPZbFWrC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# debug augmentation ----------------------------------------------\n",
        "image, mask = next(iter(train_loader))\n",
        "show_side_by_side(image[0,0,:,:].cpu(), mask[0,:,:].cpu())"
      ],
      "metadata": {
        "id": "PHiGzB2JaVdD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training Loop"
      ],
      "metadata": {
        "id": "EgphNEvQNpJA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model, val_loader, loss_fn):\n",
        "  with torch.no_grad():\n",
        "    val_loss_cum = 0.0\n",
        "    num_samples_epoch = 0\n",
        "    for batch_idx, batch in enumerate(val_loader):\n",
        "       images, masks = batch\n",
        "       # forward pass\n",
        "       outputs = model(images)\n",
        "       loss = loss_fn(y_pred=outputs, y_true=masks)\n",
        "       # compute stats\n",
        "       num_samples_batch = batch[0].shape[0]\n",
        "       num_samples_epoch += num_samples_batch\n",
        "       val_loss_cum += loss.item() * num_samples_batch\n",
        "    avg_val_loss = val_loss_cum / num_samples_epoch\n",
        "    return avg_val_loss"
      ],
      "metadata": {
        "id": "bU8MFRa7NZBn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# NN\n",
        "if (MODEL_TYPE == 'Unet'):\n",
        "  model = smp.Unet(\n",
        "      encoder_name=ENCODER_NAME,\n",
        "      encoder_depth=ENCODER_DEPTH, # between 3-5 (default=5)\n",
        "      encoder_weights=ENCODER_WEIGHTS,\n",
        "      decoder_attention_type=DECODER_ATTENTION_TYPE,\n",
        "      activation = None, # we are interested in the logits\n",
        "      classes=1\n",
        "  )\n",
        "\n",
        "if (MODEL_TYPE == 'Unet++'):\n",
        "  model = smp.UnetPlusPlus(\n",
        "      encoder_name=ENCODER_NAME,\n",
        "      encoder_depth=ENCODER_DEPTH, # between 3-5 (default=5)\n",
        "      encoder_weights=ENCODER_WEIGHTS,\n",
        "      decoder_attention_type=DECODER_ATTENTION_TYPE,\n",
        "      activation=None, # we are interested in the logits\n",
        "      classes=1\n",
        "  )\n",
        "\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "model = model.to(device)\n",
        "\n",
        "# Loss and Optimizer\n",
        "loss_fn = JaccardLoss(mode='binary', from_logits=True)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "YCEXL7zHNrBy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "globaliter = 0"
      ],
      "metadata": {
        "id": "AZrbZsWnOxlq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nbr_epochs = 5\n",
        "\n",
        "for epoch in range(nbr_epochs):\n",
        "    model.train()\n",
        "    # reset statistics trackers\n",
        "    train_loss_cum = 0.0\n",
        "    num_samples_epoch = 0\n",
        "    t = time.time()\n",
        "\n",
        "    print('batch_idx: ', end='')\n",
        "    for batch_idx, batch in enumerate(train_loader):\n",
        "        images, masks = batch\n",
        "       \n",
        "        # forward pass\n",
        "        outputs = model(images)\n",
        "        loss = loss_fn(y_pred=outputs, y_true=masks)\n",
        "    \n",
        "        # backward pass and gradient step\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # keep track of train stats\n",
        "        num_samples_batch = batch[0].shape[0]\n",
        "        num_samples_epoch += num_samples_batch\n",
        "        train_loss_cum += loss.item() * num_samples_batch\n",
        "        print(batch_idx, ' ', end='')\n",
        "\n",
        "    # after epoch, average the accumulated statistics\n",
        "    avg_train_loss = train_loss_cum / num_samples_epoch\n",
        "    # validation:\n",
        "    model.eval()\n",
        "    validation_loss = evaluate(model, val_loader, loss_fn)\n",
        "    epoch_duration = time.time() - t\n",
        "\n",
        "    if USE_WANDB:\n",
        "        wandb.log({\"train loss\": avg_train_loss, \"val_loss\": validation_loss})\n",
        "\n",
        "    print()\n",
        "    print(\n",
        "        f'Epoch: {globaliter} |'\n",
        "        f'Train loss: {avg_train_loss:.4f} |'\n",
        "        f'Validation loss: {validation_loss:.4f}|'\n",
        "        f'Duration: {epoch_duration:.2f} sec'\n",
        "    )\n",
        "\n",
        "    globaliter += 1"
      ],
      "metadata": {
        "id": "816h6vnvOtLJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if USE_WANDB:\n",
        "  wandb.finish()"
      ],
      "metadata": {
        "id": "0NOHvJiaOu48"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prediction"
      ],
      "metadata": {
        "id": "c4MYcOagUQN8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def save_single_model_pred(model, filename, data_test, processing_transform, pad_expert_images=False):\n",
        "  \"\"\"\n",
        "  Apply all the necessary preprocessing steps to a test set and predict with the given model.\n",
        "  \"\"\"\n",
        "  with torch.no_grad():\n",
        "    if pad_expert_images:\n",
        "      padder = PadIfNeeded(min_height=1007, min_width=1007, border_mode=cv2.BORDER_CONSTANT, value=0)\n",
        "    submission = []\n",
        "    for item in data_test:\n",
        "      name = item[\"name\"]\n",
        "      video = item[\"video\"]\n",
        "      shape = video[:, :, 0].shape\n",
        "      if pad_expert_images:\n",
        "        cropper= CenterCrop(shape[0], shape[1])\n",
        "        resizer2 = A.Resize(1007, 1007)\n",
        "      else:\n",
        "        resizer = A.Resize(shape[0], shape[1])\n",
        "      predictions = []\n",
        "      for idx in range(video.shape[2]):\n",
        "        frame = video[:, :, idx]\n",
        "        if pad_expert_images:\n",
        "          frame = padder(image = frame)['image']\n",
        "        processed_frame = processing_transform(image = frame)['image']\n",
        "        processed_frame = processed_frame[None, :]\n",
        "        device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "        processed_frame = processed_frame.to(device)\n",
        "        prediction = model(processed_frame)\n",
        "        prediction = prediction.cpu().detach().numpy().squeeze()\n",
        "        if pad_expert_images:\n",
        "          prediction = resizer2(image=prediction)[\"image\"]\n",
        "          prediction = cropper(image=prediction)[\"image\"]\n",
        "        else:\n",
        "          prediction = resizer(image = prediction)[\"image\"]\n",
        "        prediction = prediction > 0\n",
        "        predictions.append(prediction)\n",
        "      predictions = np.stack(predictions, axis=2)\n",
        "      submission.append({\"name\":name, \"prediction\":predictions})\n",
        "  save_zipped_pickle(submission, f\"{PATH}/{filename}.pkl\")\n",
        "\n",
        "\n",
        "def save_pred_for_ensembling(model, filename, data_test, processing_transform, pad_expert_images=False):\n",
        "    \"\"\"\n",
        "    Apply all the necessary preprocessing steps to a test set and save\n",
        "    the prediction in a format suitable for ensembling.\n",
        "    \"\"\"\n",
        "    if pad_expert_images:\n",
        "      padder = PadIfNeeded(min_height=863, min_width=863, border_mode=cv2.BORDER_CONSTANT, value=0)\n",
        "    submission = []\n",
        "    for item in data_test:\n",
        "        name = item[\"name\"]\n",
        "        video = item[\"video\"]\n",
        "        shape = video[:, :, 0].shape\n",
        "        if pad_expert_images:\n",
        "          cropper= CenterCrop(shape[0], shape[1])\n",
        "          resizer2 = A.Resize(1007, 1007)\n",
        "        else:\n",
        "          resizer = A.Resize(shape[0], shape[1])\n",
        "        predictions = torch.empty((video.shape[2], 1, shape[0], shape[1]), dtype=torch.bool)\n",
        "        for idx in range(video.shape[2]):\n",
        "            frame = video[:, :, idx]\n",
        "            if pad_expert_images:\n",
        "              frame = padder(image = frame)['image']\n",
        "            processed_frame = processing_transform(image = frame)['image']\n",
        "            processed_frame = processed_frame[None, :]\n",
        "            device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "            processed_frame = processed_frame.to(device)\n",
        "            with torch.no_grad():\n",
        "                prediction = model(processed_frame)\n",
        "            prediction = prediction.cpu().detach().numpy().squeeze()\n",
        "            if pad_expert_images:\n",
        "              prediction = resizer2(image=prediction)[\"image\"]\n",
        "              prediction = cropper(image=prediction)[\"image\"]\n",
        "            else:\n",
        "              prediction = resizer(image = prediction)[\"image\"]\n",
        "            prediction = torch.tensor(prediction[None, :])\n",
        "            prediction = prediction > 0\n",
        "            predictions[idx] = prediction\n",
        "        submission.append(predictions)\n",
        "    save_zipped_pickle(submission, f\"{PATH}/{filename}.pkl\")"
      ],
      "metadata": {
        "id": "T01ol__3UOxI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_test = load_zipped_pickle(\"{}/test.pkl\".format(PATH))\n",
        "\n",
        "processing_transform = A.Compose([   \n",
        "    A.CLAHE(p=1),\n",
        "    A.Resize(RESIZE_SIZE, RESIZE_SIZE),\n",
        "    A.ToRGB(),\n",
        "    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "    ToTensorV2()\n",
        "])\n",
        "\n",
        "save_single_model_pred(\n",
        "    model=model,\n",
        "    filename=PRED_FILE_NAME + '_individual',\n",
        "    data_test=data_test,\n",
        "    processing_transform=processing_transform,\n",
        "    pad_expert_images=PAD_EXPERT_IMAGES\n",
        ")\n",
        "\n",
        "save_pred_for_ensembling(\n",
        "    model=model,\n",
        "    filename=PRED_FILE_NAME,\n",
        "    data_test=data_test,\n",
        "    processing_transform=processing_transform,\n",
        "    pad_expert_images=PAD_EXPERT_IMAGES\n",
        ")"
      ],
      "metadata": {
        "id": "Vozmy2Q0sm3A"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}